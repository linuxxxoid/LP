# Реферат
## по курсу "Логическое программирование"

### студент: Вельтман Л.Я.

## ТЕМА

## Результат проверки

| Преподаватель     | Дата         |  Оценка       |
|-------------------|--------------|---------------|
| Сошников Д.В. |  8.12.2018            |    5           |
| Левинская М.А.|              |               |

## Почему не получилось реализовать ЭВМ пятого поколения?

Человек, несведущий в области компьютерных технологий сразу же задастся вопросом:"А что такое компьютеры пятого поколения?". Постараемся грамотно ответить на заданный вопрос. Всякое повествование начинается с исторической справки. Так не будем пренебрегать традицией и приступим.

К 1970 году Япония заняла второе место в мире по количеству внедренных ЭВМ, масштабы производства бытовой электротехники, оптических приборов, цветных телевизоров, видеомагнитофонов превосходили все ожидания. На предприятиях массово внедрялись различные роботы-манипуляторы, возросло количество автоматических станков с числовым программным управлением. Из новинок можно отметить выпуск микроэлектроники, больших и сверхбольших интегральных схем, усовершенствованное медицинское и спутниковое оборудование. Появившиеся технологии не нуждались в потреблении огромного количества электроэнергии, сырья и рабочей силы. Главенствующие позиции здесь занимали высокая точность производства и наукоемкость.
Все это подтолкнуло страну восходящего солнца к идее о переходе к совершенно новым архитектурам, направленным на разработку искусственного интеллекта. Искусственным интеллектом (ИИ) называется умение интеллектуальных машин выполнять творческие функции, которые способен выполнить только человек. Также этим термином обозначают науку и технологию создания самих интеллектуальных машин. Поэтому в 1982 году в Японии начали создание ЭВМ, которая смогла бы решать сверхсложные задачи за короткие промежутки времени, то есть иметь производительность суперкомпьютеров, управлять большими системами, но самое главное - обладать высокоразвитым искусственным интеллектом.

На данном этапе развития существовало четыре поколения компьютеров:

1. Ламповые ЭВМ.
Это программируемые цифровые компьютеры, в которых логические цепи построены на основе электронных ламп. Электронная лампа – это прибор, который работает за счет изменения потока электронов, двигающихся в вакууме от катода к аноду. Принцип работы электронной лампы следующий. Если на вход лампы подается логическая единица (например, напряжение 2 Вольта), то на выходе с лампы мы получим либо логический ноль (напряжение менее 1В), или логическую единицу (2В). Логическую единицу получим, если управляющее напряжение отсутствует, так как ток беспрепятственно пройдет от катода к аноду. Если же на сетку подать отрицательное напряжение, то электроны, идущие от катода к аноду, будут отталкиваться от сетки, и, в результате, ток протекать не будет, и на выходе с лампы будет логический ноль. Используя этот принцип, строились все логические элементы ламповых ЭВМ.

2. Транзисторные.
Новой элементной базой в таких компьютерах являлись полупроводниковые транзисторы (триоды). По сравнению с электронными лампами использование транзисторов позволило уменьшить размеры вычислительной техники, увеличив плотность монтажа, повысить надежность, увеличить скорость работы (до 1 млн. операций в секунду), понизить потребление энергии и практически искоренить теплоотдачу. Развиваются способы хранения информации: широко используется магнитная лента, позже появляются диски.

3. ЭВМ, основанные на интегральных схемах.
Интегральная схема представляет собой электронную схему, вытравленную на кремниевом кристалле. Надежность интегральных схем намного превосходит надежность аналогичных схем транзисторов. Большое влияние на это оказало уменьшением межсхемных соединений. Повышение надежности, автоматизация производства привело к значительному снижению стоимости эксплуатации ЭВМ, что позволило им проникнуть в различные сферы деятельности человека. Повышения плотности упаковки электронных схем повлекло за собой скоращение времени передачи сигнала по проводникам, таким образом, увеличилось быстродействие ЭВМ, также это уменьшило габариты и потребляемую мощность, что позволило применять такие компьютеры в авиации и космическом оборудовании.

4. Компьютеры, основанные на микропроцессорах.
Микропроцессор — это сверхбольшая интегральная схема, способная выполнять функции основного блока компьютера — процессора. Более простыми словами микропроцессор - это мозг в миниатюре, который работает по программе, заложенной в его память. Таким образом, появилась возможность размещать на одном кристалле не одну интегральную схему, а тысячи. Быстродействие устройств, собранных с применением микропроцессоров, значительно увеличилось, более того малые габариты и сравнительная дешевизна существенно отличали такие ЭВМ от своих предшественников. Самой популярной разновидностью таких машин являются персональные компьютеры, которые завоевали и продолжают завоевывать потребительский рынок.

Все эти поколения подвергались модернизации засчет минитиарюзации, то есть увеличения количества элементов на единицу площади. А по мнению Японских ученых компьютеры пятого поколения должны были строиться по принципу совмещения неограниченного количества микропроцессоров для достижения сверхпроизводительности, а не просто обладать микрокомпонентами. Кампания по разработке машин нового поколения подразумевала изменение и усовершенствование методов программирования, технических подходов к решению обыденных задач, прорыв в развитии научного направления в области искусственного интеллекта.

Предполагалось, что ЭВМ 5-го поколения будет иметь развитую периферийную систему, интеллектуальный интерфейс, которые позволят различать звуковую, зрительную, сенсорную информацию, то есть общение между компьютером и человеком перейдет на новый уровень. Взаимодействие будет происходить на обычном человеческом языке. Такие компьютеры должны воспринимать информацию с рукописного или печатного текста, с человеческого голоса, узнавать пользователя по голосу, осуществлять перевод с одного языка на другой со 100% точностью. Это позволило бы общаться с ЭВМ всем пользователям, даже тем, кто не обладает специальными знаниями в этой области. Такой суперкомпьютер мог быть создан на основе моделирования нашего мозга, что не исключает возможности самоорганизации и самоусовершенствования. Для создания таких ЭВМ нужно было перейти к новой технологической базе намного отличавшейся от баз компьютеров прошлых поколений. Если перед разработчиками машин с I по IV поколений стояли такие цели, как увеличение производительности в области числовых расчётов, достижение большой ёмкости памяти, то основной задачей разработчиков ЭВМ V поколения является создание искусственного интеллекта машины, развитие "интеллектуализации" компьютеров - устранения барьера между человеком и компьютером. 

Японские ученые определили основные требования к компьютерам 5-го поколения:

1) Создание развитого человеко-машинного интерфейса (распознавание речи, образов);
2) Развитие логического программирования для создания баз знаний и систем искусственного интеллекта;
3) Создание новых технологий в производстве вычислительной техники;
4) Создание новых архитектур компьютеров и вычислительных комплексов.


Также были озвучены и главные направления исследований:

1) Технологии логических заключений для обработки знаний.
2) Технологии для работы со сверхбольшими базами данных и базами знаний.
3) Рабочие станции с высокой производительностью.
4) Компьютерные технологии с распределёнными функциями.
5) Суперкомпьютеры для научных вычислений.

Сопоставляя все эти данные, Япония пришла к выводу, что им нужен компьютер с параллельными процессорами, работающий с данными, хранящимися в обширной базе данных, а не в файловой системе. При этом получение данных подразумевалось с использованием языка логического программирования. Предполагалось, что прототип машины будет обладать производительностью между 100 млн и 1 млрд LIPS, где LIPS — это логическое заключение в секунду. К тому времени типовые рабочие станции были способны на производительность около 100 тысяч LIPS. Планировалось, что в ходе разработок компьютерный интеллект, набирая мощность, начинает изменять сам себя, и одной из главных целей было создать такую компьютерную среду, которая сама начнёт производить следующую, причём принципы, на которых будет построен окончательный компьютер, были заранее неизвестны, их нужно было выработать в процессе использования стартовых компьютеров. Далее, для резкого увеличения производительности, предлагалось постепенно заменять программные решения аппаратными, поэтому не делалось резкого разделения между задачами для программной и аппаратной базы. Ожидалось добиться существенного прорыва в области решения прикладных задач искусственного интеллекта.

Таким образом, предлагаемая в проекте ЭВМ пятого поколения технология подготовки прикладных задач к решению на компьютере включает два этапа:

1) программист создает пустую программную оболочку;
2) заказчик (конечный пользователь) наполняет оболочку знаниями

База знаний пополняется самим конечным пользователем, и такая наполненная программная оболочка способна решать прикладные задачи, описанные заказчиком правилами. То есть, начинается эксплуатация программного продукта. Данная технология имеет множество существенных проблем, связанных с представлением и манипулированием знаниями. Тем не менее, с ней связывают прорыв в области проектирования прикладных программных продуктов.

### Проблемы.

Однако, следующие десять лет проект ЭВМ 5-го поколения стал испытывать ряд трудностей разного типа.
Первая проблема заключалась в том, что язык Пролог, выбранный за основу проекта, не поддерживал параллельных вычислений, которые составляли основу компьютера будущего. Пролог - декларативный язык программирования, воспринимает в качестве программы некоторое описание задачи или баз знаний и сам производит логический вывод, а также поиск решения задач, пользуясь механизмом поиска с возвратом и унификацией. Интерес к этому языку был наиболее ярко выражен как раз-таки во время разработок японской национальной программы ЭВМ 5 поколения в 1980-х годах. Разработчики думали, что с помощью Пролога можно сформулироввать новые принципы, которые приведут к созданию компьютеров более высокого уровня интеллекта. 
Зачастую то, что на стадии идеи звучит выполнимо, в реальности происходит наоборот, поэтому пришлось разрабатывать собственный язык, способный работать в мультипроцессорной среде. Это обернулось трудной задачей — было предложено несколько языков, каждый из которых обладал собственными ограничениями.

Другая проблема возникла с производительностью процессоров. Оказалось, что технологии 80-х годов стремительно обогнали все те трудности и ограничения, которые перед началом проекта считались «очевидными» и непреодолимыми. Параллелизм многих процессоров не вызывал ожидаемого резкого подъема производительности. Вышло так, что рабочие станции, которые создавались непосредственно ради проекта, успешно достигли и даже превзошли требуемые мощности, но к этому времени появились коммерческие компьютеры, которые были ещё мощнее.

Более того, проект «Компьютеры пятого поколения» оказался провальным, рассматривая лишь технологию производства программного обеспечения. Задолго до старта разработки этого проекта фирма Xerox разработала экспериментальный графический интерфейс (GUI). Чуть позже создали Интернет, и зародилась совершенно новая концепция распределения и хранения данных, при этом поисковые машины привели к новому качеству хранения и доступа различной информации. Не осталось никакой надежды на развитие логического программирования, на которую рассчитывали японцы в проекте «Компьютеры пятого поколения». Все это оказались неосуществимым, преимущественно по причине ограниченности ресурсов и ненадёжности технологий.

Концепция саморазвития системы, по которой предполагалось, что она сама должна менять свои внутренние правила и параметры, оказалась непродуктивной и неэффективной. Такая технология достигала своего максимума и неизбежно терпела крах, так как теряла надежность и цельность, наблюдались непредвиденные ситуации, в целом система "глупела" и вела себя неадекватно.

Несвоевременной оказалась и идея широкомасштабной замены программных средств аппаратными. В будущем компьютеризация пошла решительно по противоположному направлению, развивая программные средства при помощи более примитивных, но стандартных аппаратных средств, все задачи перекладывались на программное обеспечение. Таким образом, проект был поставлен в рамки мышления 1970-х годов и не смог претворить в жизнь чёткое разграничение функций программной и аппаратной части компьютеров.

Анализируя сложившуюся ситуацию с японским проектом, его можно считать абсолютным фиаско. Разработка длилась на протяжении десяти лет, было потрачено более $ 500 000 000. Программу свернули, так и не достигнув поставленной цели. Не получилось вывести на рынок и рабочие станции, потому что однопроцессорные системы других фирм превосходили их по многим характеристикам, программные системы так и не заработали, к тому же появление Интернета сделало все идеи проекта безнадёжно устаревшими.

Cуществует ряд объективных и субъективных факторов, которыми объясняют все неудачи проекта:

1) При оценке тенденций развития компьютеров были допущены грубейшие ошибки. Дальнейшая судьба развития аппаратных средств была катастрофически недооценена, и, наоборот, излишнюю надежду возлагали на перспективы искусственного интеллекта, многие из планируемых задач ИИ так и не нашли эффективного коммерческого решения до сих пор, в то время как мощность компьютеров несоизмеримо выросла;
2) неверная стратегия, связанная с разделением задач, решаемых программно и аппаратно. При такой тактике возникло стремление к постепенной замене программных средств аппаратными, что неминуемо привело к чрезмерному усложнению аппаратных средств;
3) отсутствие опыта и глубинного понимания специфики данной области, недостаток навыков в решении задач искусственного интеллекта. Японские ученые надеялись на самоорганизацию системы при увеличении её производительности, а также вследствие подчинения этой системы неким базовым принципам, которые так и не были сформулированы;
4) трудности, выявившиеся по мере исследования реального ускорения, которое получает система логического программирования при запараллеливании процессоров. Проблема состоит в том, что в многопроцессорной системе, построенной на основе Uniform Memory Access, резко увеличиваются затраты на связь между отдельными процессорами, которые практически уравнивают выгоду от параллелизации операций, отчего с какого-то момента добавление новых процессоров почти не улучшает производительности системы;
5) неправильный выбор языков типа Лисп и Пролог для создания базы знаний и манипулирования данными. В 1980-е годы эти системы программирования пользовались популярностью для САПР и экспертных систем, однако эксплуатация показала, что приложения оказываются малонадёжными и плохо отлаживаемыми по сравнению с системами, разработанными обычными технологиями, отчего от этих идей пришлось отказаться. Кроме того, непосильной задачей оказалась и реализация «параллельного Пролога», которая так и не была успешно решена;
6) низкий общий уровень технологии программирования того времени и диалоговых средств;
7) чрезмерная рекламная кампания проекта «национального престижа». Люди тех годов верили в будущее параллельных вычислений, поэтому проект «компьютеров пятого поколения» имел исключительный резонанс и был принят в компьютерном мире очень серьёзно. После того, как Япония показала себя вполне конкурентноспособным производителем промышленности, она приобрела хорошую репутацию. Проекты в области параллельной обработки данных тут же начали разрабатывать в США — в Корпорации по Микроэлектронике и Компьютерной Технологии (MCC), в Великобритании — в фирме Олви (Alvey), и в Европе в рамках Европейской Стратегической Программы Исследований в области Информационных Технологий (ESPRIT). В СССР предприняли попытку не отстать от западных стран. Было принято решение разработать свой собственный вариант ЭВМ пятого поколения. Будущий многопроцессорный компьютер был назван "МАРС". Но уже тогда отставание от японцев в области микроэлектроники было на 10-15 лет. Весь проект основывался на устаревших инженерно-технических решениях и архаических языках программирования типа Модула-2. Единственным отличием от остальных ЭВМ была мультипроцессорность. Как итог, данная машина не соответствовала определению: «компьютер пятого поколения».

### Настоящее время.

На сегодняшний день компьютеры достигли такого уровня, на котором стало возможным считывать информацию с рукописного, печатного текста, с человеческого голоса, распознавать речь, переводить данные с разных языков. Каждые 3-5 лет в несколько раз увеличивается степень интеграции электронных схем, модернизируется технология их производства, а следовательно это способствует снижению стоимости компонентов компьютера. Сетевые технологии позволяют связывать компьютеры в локальные и глобальные сети, которые образуют глобальную паутину — Интернет. Типичный объем оперативной памяти современных персональных компьютеров — сотни мегабайт, дисковой памяти — десятки или сотни гигабайт, тактовая частота — единицы гигагерц. Это дает огромные преимущества всем людям, которые пользуются данным функционалом, не имея при этом каких-то специальных познаний в этой области. 

Уже появились процессоры, которые используют свет как носитель информации, такие процессоры называются оптическими. В 2003 году был выпущен оптический процессор Enligh256, который имеет оптическое ядро, а входные и выходные данные представлены в электронном виде. Производительность у такого компьютера 8 триллионов операций в секунду. Его применяют в военной технике и при обработке видеоданных в реальном времени. Оптоволокно намного тоньше и быстрее по сравнению с электрическими проводниками. И ведь на самом деле, применение электронных коммутаторов ограничивает быстродействие сетей примерно 50 Гбит/с. Чтобы достичь терабитных скоростей потребуются оптические коммутаторы. Сегодня уже появились опытные образцы. Нетрудно догадаться, почему в телекоммуникациях побеждает оптоволокно: тысячекратное увеличение пропускной способности, причем мультиплексирование позволяет повысить ее еще больше. Инженеры пропускают по оптоволокну все больше и больше коротковолновых световых лучей. В последнее время для управления ими применяются чипы типа TI DMD с сотнями тысяч микрозеркал. Если первые трансатлантические медные кабели позволяли передавать всего 2500 Кбит/с, то первое поколение оптоволоконных кабелей - уже 280 Мбит/с. Кабель, проложенный сейчас, имеет теоретический предел пропускной способности в 10 Гбит/с на один световой луч определенной длины волны в одном оптическом волокне.

Недавно компания Quest Communications проложила оптический кабель с 96 волокнами, 48 из них она заняла для своих потребностей, причем по каждому волокну может пропускаться до восьми световых лучей с разной длиной волны. Возможно, что при дальнейшем развитии технологии мультиплексирования число лучей увеличится еще больше, что позволит расширять полосу пропускания без замены кабеля.

Целиком оптические компьютеры появятся через десятилетия, но работа в этом направлении идет сразу на нескольких фронтах. Известно, что ученые из университета Торонто создали молекулы жидких кристаллов, управляющие светом в фотонном кристалле на базе кремния. Они считают возможным создание оптических ключей и проводников, способных выполнять все функции электронных компьютеров.

Прежде чем оптические компьютеры станут войдут в массовое использование, на оптические компоненты, вероятно, перейдет вся система связи. В ближайшие 15 лет оптические коммутаторы, повторители, усилители и кабели заменят электрические компоненты.

Большие надежды связаны с разработкой квантовых компьютеров, в которых применяются идеи квантовой физики. Квантовый компьютер будет состоять из компонентов субатомного размера и работать по принципам квантовой механики. Квантовый мир - очень странное место, в котором объекты могут занимать два разных положения одновременно, это явление получило название квантовой суперпозиции. Но именно этот факт и открывает новые возможности. Данные для обработки записываются в систему кубитов – квантовых битов. Кубит вмещает в себе больше информации, чем бит. Поэтому, для задач, где не хватает вычислительных ресурсов (например, раскрытие шифров), квантовый компьютер будет успешно использован.

Также ведется разработка биологических компьютеров, которые работают как живой организм. Ячейки памяти такого компьютера – это молекулы сложных органических соединений (например, ДНК). Процесс вычисления – это химическая реакция, а результат – состав и строение получившейся молекулы. Использование биологических материалов в вычислительной технике позволит уменьшить компьютеры до размеров живой клетки. Но пока эта всего лишь чашка Петри, наполненная спиралями ДНК, или нейроны, взятые у пиявки и подсоединенные к электрическим проводам. В Технологическом институте штата Джорджия Билл Дитто провел интересный эксперимент, подсоединив микродатчики к нескольким нейронам пиявки. Он обнаружил, что в зависимости от входного сигнала нейроны образуют новые взаимосвязи. Вероятно, биологические компьютеры, состоящие из нейроподобных элементов, в отличие от кремниевых устройств, смогут искать нужные решения посредством самопрограммирования. Дитто намерен использовать результаты своей работы для создания мозга роботов будущего. По существу, наши собственные клетки - это не что иное, как биомашины молекулярного размера, а примером биокомпьютера, конечно, служит наш мозг.

В области нанотехнологий также проводятся исследования по созданию транзистора, размером с молекулу. Стало известно, что компания Hewlett-Packard объявила о том, что они справились с задачей изготовления компонентов, из которых могут быть построены мощные молекулярные компьютеры. HP и Калифорнийский университет в Лос-Анджелесе (UCLA) сообщили, что им удалось заставить молекулы ротаксана переходить из одного состояния в другое - по сути, это означает создание молекулярного элемента памяти.
Следующим заданием будет изготовление логических ключей, способных выполнять функции И, ИЛИ и НЕ. Составляющими такого компьютера будет слой проводников, проложенных в одном направлении, слой молекул ротаксана и слой проводников, направленных в обратную сторону. Взаиморасположение компонентов, состоящих из необходимого числа ячеек памяти и логических ключей, создается электронным способом. По оценкам ученых HP, подобный компьютер будет в 100 млрд раз экономичнее современных микропроцессоров, занимая при этом намного меньше места.
Сама идея этих логических элементов не является революционной: кремниевые микросхемы содержат миллиарды таких же. Но преимущества в потребляемой энергии и размерах способны сделать компьютеры вездесущими. Молекулярный компьютер размером с песчинку может содержать миллиарды молекул. А если научиться делать компьютеры не трехслойными, а трехмерными, преодолев ограничения процесса плоской литографии, применяемого для изготовления микропроцессоров сегодня, преимущества станут еще больше.
Кроме того, молекулярные технологии обещают появление микромашин, способных перемещаться и прилагать усилие. Причем для создания таких устройств можно применять даже традиционные технологии травления. Когда-нибудь эти микромашины будут самостоятельно заниматься сборкой компонентов молекулярного или атомного размера.
Первые опыты с молекулярными устройствами еще не гарантируют появления таких компьютеров, однако это именно тот путь, который предначертан всей историей предыдущих достижений.

Если на каком-нибудь из этих направлений удастся добиться успеха, то компьютеры могут стать вездесущими. А если таких успешных направлений будет несколько, то они распределятся по разным нишам. Например, квантовые компьютеры будут специализироваться на шифровании и поиске в крупных массивах данных, молекулярные - на управлении производственными процессами и микромашинах, а оптические - на средствах связи.


### Вывод

К сожалению, японский проект ЭВМ пятого поколения ретранслировал трагическую участь ранних исследований в области искусственного интеллекта. Более 500 миллионов долларов инвестиций были потрачены зря, проект закрыт, а по производительности такие устройства недалеко ушли вперед от массовых систем тех годов.

В наше время определение "компьютер пятого поколения" является неопределенным и применяется во многих смыслах, например, при описании систем облачных вычислений. Однако, есть и плюсы проведенных в ходе проекта исследований, накопленный опыт по методам представления знаний и параллельного логического вывода сильно помогли прогрессу в области систем искусственного интеллекта.

В настоящий момент разработчики нашей планеты вынуждены действовасть весьма нерациональным способом для достижения прогресса, а именно, используя метод проб и ошибок. Пока это единственный путь создания ЭВМ следующих поколений. Последующие революционные компьютеры будут развивать посредством улучшения функциональных характеристик, создания новых парадигм и теорий, которые будут базироваться на "мыслительных процессах", оперировать образами объективного и субъективного мира.

Остается надеяться, что сегодняшие ученые учтут ошибки Японии 1980-х годов, рассмотрят задачу со всех сторон, учитывая все недостатки и преимущества, правильно прогнозируя возможные варианты событий, чтобы избежать повторения судьбы японского проекта ЭВМ пятого поколения.
